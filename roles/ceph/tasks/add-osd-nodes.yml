- name: Cleanup the NVME Devices
  shell: |
    pvremove -y -ff {{ item }}
    dd if=/dev/zero of={{ item }} bs=4096 count=1 conv=notrunc
    nvme format {{ item }}
    wipefs -a {{ item }}
    sgdisk --zap-all --delete {{ item }}
    blkdiscard {{ item }}
  register: cleanup_ceph_block_devices
  # ignore_errors: true
  with_items: "{{ nvme_device_array }}"
  when: inventory_hostname in groups['osd']
- debug: msg={{ cleanup_ceph_block_devices }}
  when: inventory_hostname in groups['osd'] and print_debug == true


- name: Get FSID
  shell: |
    {{ ceph.base_path }}/cephadm ls | grep fsid | uniq | awk '{print $2}' | sed -e "s/\"//g" | cut -d , -f 1
  register: get_fsid
  when: inventory_hostname in groups['control']
- debug: msg={{ get_fsid.stdout }}
  when: print_debug == true and inventory_hostname in groups['control']


- name: Set Fact for FSID
  set_fact:
    _fsid: "{{ get_fsid.stdout }}"
  delegate_to: "{{ item }}"
  delegate_facts: True
  with_items: "{{ groups['all'] }}"
  when: inventory_hostname in groups['control']


- name: Set Fact for cephadm command
  set_fact:
    cephadm_cmd: "/usr/sbin/cephadm shell --fsid {{ _fsid }} -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --"
  delegate_to: "{{ item }}"
  delegate_facts: True
  with_items: "{{ groups['all'] }}"
  when: inventory_hostname in groups['control']


- name: Add the OSD Nodes to the Ceph Cluster
  shell: |
    {{ cephadm_cmd }} ceph orch host add {{ hostvars[item]['ansible_hostname'] }}
  register: osd_nodes_added
  with_items: "{{ groups['osd'] }}"
  when: inventory_hostname in groups['control']
- debug: msg={{ osd_nodes_added }}
  when: print_debug == true and inventory_hostname in groups['control']


- pause:
    minutes: 2


- name: List Ceph Cluster Nodes
  shell: |
    {{ cephadm_cmd }} ceph orch host ls
  register: ceph_nodes_listed
  when: inventory_hostname in groups['control']
- debug: msg={{ ceph_nodes_listed }}
  when: print_debug == true and inventory_hostname in groups['control']


- name: List the Devices that are Available on the OSD Nodes for Creating OSDs Using the Below Command
  shell: |
    {{ cephadm_cmd }} ceph orch device ls
  register: devices_listed
  when: inventory_hostname in groups['control']
- debug: msg={{ devices_listed }}
  when: print_debug == true and inventory_hostname in groups['control']


- pause:
    minutes: 2


# https://docs.ceph.com/en/quincy/cephadm/services/osd/
- name: Disable the Automatic Creation of OSDs on Avaialble Devices
  shell: |
    {{ cephadm_cmd }} ceph orch apply osd --all-available-devices --unmanaged=true
  register: auto_creation_disabled
  when: inventory_hostname in groups['control']
- debug: msg={{ auto_creation_disabled }}
  when: print_debug == true and inventory_hostname in groups['control']
# Automatic attach all devices at Once
# {{ cephadm_cmd }} ceph orch apply osd --all-available-devices --method {raw|lvm}
# {{ cephadm_cmd }} ceph config set osd osd_memory_target_autotune true


- name: Maually Create OSDs with Specific Devices on Specific Hosts
  shell: |
    {{ cephadm_cmd }} ceph orch daemon add osd \
    {{ hostvars[inventory_hostname]['ansible_hostname'] }}:data_devices={{ nvme_device_list }}
    sleep 60
  delegate_to: "{{ hostvars[groups['control'][0]]['ansible_hostname'] }}"
  delegate_facts: true
  register: manual_osds_created
  when: inventory_hostname in groups['osd']
- debug: msg={{ manual_osds_created }}
  when: print_debug == true and inventory_hostname in groups['osd']


- name: Check if OSDs are still Down
  shell: |
    ceph osd df | sed 1d | awk '{print" "$NF}' | sed 's/[[:space:]]//g' | grep down | uniq
  register: check_osd_down
  until: check_osd_down.stdout.find("") != -1
  retries: 100
  delay: 20
  ignore_errors: true
  # changed_when: check_osd_down.rc != 1 and check_osd_down.rc != 0
  when: inventory_hostname in groups['control']
- debug: msg={{ check_osd_down }}
  when: print_debug == true and inventory_hostname in groups['control']


#- name: Up OSDs
#  shell: |
#    {{ cephadm_cmd }} ceph osd up all
#  register: osd_up
#  ignore_errors: true
#  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
#- debug: msg={{ osd_up }}
#  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


#- meta: end_play
- pause:
    minutes: 2


- name: Label the OSD Nodes with It's Role
  shell: |
    {{ cephadm_cmd }} ceph orch host label add {{ hostvars[item]['ansible_hostname'] }} {{ hostvars[item]['ansible_hostname'] }}-osd
  register: osd_nodes_labeled
  loop: "{{ groups['osd'] }}"
  when: inventory_hostname in groups['control']
- debug: msg={{ osd_nodes_labeled }}
  when: print_debug == true and inventory_hostname in groups['control']


- name: Check Ceph Cluster Health
  shell: |
    {{ cephadm_cmd }} ceph -s
  register: ceph_health_checked
  when: inventory_hostname in groups['control']
- debug: msg={{ ceph_health_checked }}
  when: print_debug == true and inventory_hostname in groups['control']


- name: Check OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd tree
  register: osd_tree_checked
  when: inventory_hostname in groups['control']
- debug: msg={{ osd_tree_checked }}
  when: print_debug == true and inventory_hostname in groups['control']


- name: Get a List of Ceph Services
  shell: |
    {{ cephadm_cmd }} ceph orch ps
  register: ceph_services_checked
  when: inventory_hostname in groups['control']
- debug: msg={{ ceph_services_checked }}
  when: print_debug == true and inventory_hostname in groups['control']


- name: Check Created Containers
  shell: |
    podman ps
  register: containers_checked
  when: inventory_hostname in groups['control']
- debug: msg={{ containers_checked }}
  when: print_debug == true and inventory_hostname in groups['control']


- name: List the Containers if Using Podman
  shell: |
    systemctl list-units 'ceph*'
  register: containers_listed
  when: inventory_hostname in groups['control']
- debug: msg={{ containers_listed }}
  when: print_debug == true and inventory_hostname in groups['control']


- meta: end_play


- name: Get All OSDs not UP
  shell: |
    ceph osd df | sed 1d | head -n -2 | awk '{ if($NF!="up") print $1 }' | sort -nr | tr '\n' ',' | sed 's/,$/\n/'
  register: all_osd_down_ids
  when: inventory_hostname in groups['control']
- debug: msg={{ all_osd_down_ids }}
  when: print_debug == true and inventory_hostname in groups['control']
# ceph osd df | sed 1d | head -n -2 | awk '{print $1","$20}'


- set_fact:
    all_osds: "{{ all_osd_down_ids.stdout | split(',')}}"
  when: inventory_hostname in groups['control'] and ( all_osd_down_ids.stdout | length > 0 )
- debug: msg={{ item }}
  with_items: "{{ all_osds }}"
  when: print_debug == true and inventory_hostname in groups['control'] and ( all_osd_down_ids.stdout | length > 0 )


- name: Reconfig and Redeploy Node & Ceph Exporters and Crash
  shell: |
    {{ cephadm_cmd }} ceph orch daemon reconfig crash.{{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch daemon redeploy crash.{{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch daemon reconfig ceph-exporter.{{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch daemon redeploy ceph-exporter.{{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch daemon reconfig node-exporter.{{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch daemon redeploy node-exporter.{{ hostvars[item]['ansible_hostname'] }}
  register: node_exporter_reconfig
  with_items: "{{ groups['osd'] }}"
  when: inventory_hostname in groups['control'] and ( all_osd_down_ids.stdout | length > 0 )
- debug: msg={{ node_exporter_reconfig }}
  when: print_debug == true and inventory_hostname in groups['control'] and ( all_osd_down_ids.stdout | length > 0 )


- name: Reconfig and Redeploy OSDs
  shell: |
    {{ cephadm_cmd }} ceph orch daemon reconfig osd.{{ item }}
    {{ cephadm_cmd }} ceph orch daemon redeploy osd.{{ item }}
  register: osd_reconfig
  with_items: "{{ all_osds }}"
  when: inventory_hostname in groups['control'] and ( all_osd_down_ids.stdout | length > 0 )
- debug: msg={{ osd_reconfig }}
  when: print_debug == true and inventory_hostname in groups['control'] and ( all_osd_down_ids.stdout | length > 0 )

# ceph config dump | grep -E '^osd\.' | grep iops_ssd | awk '{print $1}' | cut -d . -f 2 | sort -nr | tr '\n' ',' | sed 's/,$/\n/'
# ceph osd status | grep -E '{{ all_osd_nodes }}' | sed 1d | awk '{print $1}' | sort -nr | tr '\n' ',' | sed 's/,$/\n/'
# ceph osd status | sed 1d | awk '{print $1}' | sort -nr | tr '\n' ',' | sed 's/,$/\n/'

- name: Validate if OSDs are still Down
  shell: |
    ceph osd df | sed 1d | head -n -2 | awk '{ if($NF=="down") print $NF }' | uniq
  register: osd_down_status
  ignore_errors: true
  until: osd_down_status.stdout.find("down") == -1
  retries: 20
  delay: 10
  when: inventory_hostname in groups['control'] and ( all_osd_down_ids.stdout | length > 0 )
- debug: msg={{ osd_down_status }}
  when: print_debug == true and inventory_hostname in groups['control'] and ( all_osd_down_ids.stdout | length > 0 )

