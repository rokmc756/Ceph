#
- name: Get fsid for setting fact
  shell: |
    cat /etc/ceph/ceph.conf  | grep fsid | awk '{print $3}'
  register: get_fsid

- debug: msg={{ get_fsid.stdout }}
  when: print_debug == true

#
- name: Set fact for fsid
  set_fact:
    _fsid: "{{ get_fsid.stdout }}"

#
- name: Set fact for cephadm_cmd command
  set_fact:
    cephadm_cmd: "/usr/sbin/cephadm shell --fsid {{ _fsid }} -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --"

#
- name:  Remove OSDs
  shell: |
    cephadm shell -- ceph osd down {{ item }} && cephadm shell -- ceph osd destroy {{ item }} --force
  register: remove_osd
  with_sequence: start=0 end=8
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- debug: msg={{ remove_osd }}
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname'] and print_debug == true

#
- name: Remove the cluster hosts and check if there is ceph daemon running
  shell: |
    cephadm shell -- ceph orch host rm {{ hostvars[item]['ansible_hostname'] }}
    cephadm shell -- ceph orch host drain {{ hostvars[item]['ansible_hostname'] }}
    cephadm shell -- ceph orch ps {{ hostvars[item]['ansible_hostname'] }}
  register: remove_cluster_hosts
  with_items: "{{ groups['mon'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- debug: msg={{ remove_cluster_hosts }}
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname'] and print_debug == true

#
- name: Remove the ceph storage cluster
  shell: |
    cephadm rm-cluster --fsid {{ fsid01 }} --force
    cephadm ls
  register: remove_ceph_storage
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

- debug: msg={{ remove_ceph_storage }}
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname'] and print_debug == true

#
- name: Remove the ceph package repository
  become: true
  become_user: root
  shell: |
    cephadm rm-repo
  register: remove_ceph_repo

- debug: msg={{ remove_ceph_repo }}
  when: print_debug == true

- name: Cleanup the ceph configuration files
  shell: |
    rm -rf /var/log/ceph /var/run/ceph /var/lib/ceph /run/ceph /etc/ceph
  register: cleanup_ceph_config
  when: inventory_hostname in groups['all']

- debug: msg={{ cleanup_ceph_config }}
  when: print_debug == true

#
- name: Cleanup the ceph block devices
  shell: |
    lsblk
    for j in $(echo 'nvme0n1 nvme0n2 nvme0n3 nvme0n4'); do dd if=/dev/zero of=/dev/$j bs=1M count=1000; done
  register: cleanup_ceph_block_devices
  when: inventory_hostname in groups['osd']

- debug: msg={{ cleanup_ceph_block_devices }}
  when: inventory_hostname in groups['osd'] and print_debug == true

#
- name: Kill processes of conmon and podman
  shell: |
    killall conmon
    killall podman
  register: kill_conmon_podman
  ignore_errors: true
  when: inventory_hostname in groups['all']

- debug: msg={{ kill_conmon_podman }}
  when: inventory_hostname in groups['all'] and print_debug == true

#
- name: Uninstall conmon package
  yum:
    name: "{{ item }}"
    state: absent
  with_items:
    - cephadm
    - ceph-common
    - ceph-base
    - ceph-radosgw
    - conmon
    - podman
  when: inventory_hostname in groups['all']


#- name: Reboot required  ( Red Hat ) - Step 1 in RHEL/CentOS/Rocky Linux 8.x and 9.x
#  command: /usr/bin/needs-restarting -r
#  register: reboot_required
#  changed_when: False
#  when: ( ansible_distribution == "RedHat" or ansible_distribution == "CentOS" or ansible_distribution == "Rocky" ) and hostvars[inventory_hostname].ansible_distribution_major_version|int >= 8


- name: Reboot Required
  shell: ( /bin/sleep 5 ; /sbin/shutdown -r now "Ansible updates triggered" ) &
  async: 120
  poll: 0
  notify:
    - Waiting for Server to Come Back after Restart
  when: ( ansible_distribution == "RedHat" or ansible_distribution == "CentOS" or ansible_distribution == "Rocky" )
  # and reboot_required.rc == 1

