# vi force-destroy-ceph.sh
# https://www.flamingbytes.com/blog/how-to-uninstall-ceph-storage-cluster/


# Remove rbd-pool
# Check the pools, images and OSDs
# ceph osd tree
# ceph osd lspool
# rbd showmapped
# rbd unmap /dev/rbd[0-3]
# rbd rm datapool/rbdvol1


- name: Remove Block Device Image in {{ rbd.pool_name }} Pool
  shell: |
    rbd rm {{ rbd.custom_image  }} --pool {{ rbd.pool_name }}
  register: remove_custom_rbd_image
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- debug: msg={{ remove_custom_rbd_image }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


# Error EPERM: pool deletion is disabled;
# you must first set the mon_allow_pool_delete config option to true  before you can destroy a pool
- name: Remove custom pool
  shell: |
    ceph tell mon.\* injectargs '--mon-allow-pool-delete=true'
    ceph osd pool rm {{ rbd.pool_name }} {{ rbd.pool_name }} --yes-i-really-really-mean-it
  register: remove_custom_pool
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

- debug: msg={{ remove_custom_pool }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


# Remove NFS Cluster
- name: Delete CephFS Volume in NFS
  shell: |
    ceph fs volume rm {{ nfs.cephfs_name }} --yes-i-really-mean-it
    # cephadm shell -- ceph fs volume rm {{ nfs.cephfs_name }} --yes-i-really-mean-it
  register: delete_cephfs_volume_nfs
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- debug: msg={{ delete_cephfs_volume_nfs }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- name: Delete Ceph File System NFS Exports
  shell: |
    ceph nfs export rm {{ nfs.cephfs_name }} /ceph
    # cephadm shell -- ceph nfs export rm {{ nfs.cephfs_name }} /ceph
  register: delete_cephfs_nfs_exports
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- debug: msg={{ delete_cephfs_nfs_exports }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- name: Remove NFS Cluster
  shell: |
    ceph nfs cluster rm {{ nfs.cluster_name }}
    # cephadm shell -- ceph nfs cluster rm {{ nfs.cluster_name }}
  register: remove_nfs_cluster
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

- debug: msg={{ remove_nfs_cluster }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


# Remove Rados Gateway
- name: Remove user in the Ceph Object Gateway
  shell: |
    radosgw-admin user rm --uid=jmoon --purge-data
  register: remove_rgw_user
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- debug: msg={{ remove_rgw_user }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


#- name: Remove zone in the Ceph Object Gateway
#  shell: |
#    cephadm shell -- ceph orch rm rgw.test_realm.test_zone_bb
#  register: remove_rgw


#- debug: msg={{ remove_rgw }}
#  when: print_debug == true


- name: Delete CephFS in Pool
  shell: |
    cephadm shell -- ceph fs volume rm {{ pool.cephfs_name }} --yes-i-really-mean-it
  register: delete_cephfs_pool
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- debug: msg={{ delete_cephfs_pool }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- name: Delete volume in Pool
  shell: |
    rbd rm {{ pool.pg_name }}/{{ pool.pg_custom_volume_name }}
  register: remove_rgw
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- debug: msg={{ remove_rgw }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- name: Delete Pool
  shell: |
    cephadm shell -- ceph tell mon.* injectargs --mon_allow_pool_delete true
    cephadm shell -- ceph osd pool delete {{ pool.pg_name }} {{ pool.pg_name }} --yes-i-really-really-mean-it
  register: delete_pool
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

- debug: msg={{ delete_pool }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

- meta: end_play


- name: Get fsid for setting fact
  shell: |
    cephadm ls | grep fsid | uniq | awk '{print $2}' | sed -e "s/\"//g" | cut -d , -f 1
  register: get_fsid
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- debug: msg={{ get_fsid.stdout }}
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname'] and print_debug == true


- name: Set fact for fsid
  set_fact:
    fsid01: "{{ get_fsid.stdout }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- name:  Remove OSDs
  shell: |
    cephadm shell -- ceph osd down {{ item }} && cephadm shell -- ceph osd destroy {{ item }} --force
  register: remove_osd
  with_sequence: start=0 end=8
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- debug: msg={{ remove_osd }}
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname'] and print_debug == true


- name: Remove the cluster hosts and check if there is ceph daemon running
  shell: |
    cephadm shell -- ceph orch host rm {{ hostvars[item]['ansible_hostname'] }}
    cephadm shell -- ceph orch host drain {{ hostvars[item]['ansible_hostname'] }}
    cephadm shell -- ceph orch ps {{ hostvars[item]['ansible_hostname'] }}
  register: remove_cluster_hosts
  with_items: "{{ groups['mon'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']


- debug: msg={{ remove_cluster_hosts }}
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname'] and print_debug == true


- name: Remove the ceph storage cluster
  shell: |
    cephadm rm-cluster --fsid {{ fsid01 }} --force
    cephadm ls
  register: remove_ceph_storage
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

- debug: msg={{ remove_ceph_storage }}
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname'] and print_debug == true


- name: Remove the ceph package repository
  become: true
  become_user: root
  shell: |
    cephadm rm-repo
  register: remove_ceph_repo


- debug: msg={{ remove_ceph_repo }}
  when: print_debug == true


- name: Cleanup the ceph configuration files
  shell: |
    rm -rf /var/log/ceph /var/run/ceph /var/lib/ceph /run/ceph /etc/ceph
  register: cleanup_ceph_config
  when: inventory_hostname in groups['all']

- debug: msg={{ cleanup_ceph_config }}
  when: print_debug == true


- name: Cleanup the ceph block devices
  shell: |
    lsblk
    for j in $(echo 'vdb vdc vdd'); do dd if=/dev/zero of=/dev/$j bs=1M count=1000; done
  register: cleanup_ceph_block_devices
  when: inventory_hostname in groups['osd']

- debug: msg={{ cleanup_ceph_block_devices }}
  when: inventory_hostname in groups['osd'] and print_debug == true


- name: Kill processes of conmon and podman
  shell: |
    killall conmon
    killall podman
  register: kill_conmon_podman
  ignore_errors: true
  when: inventory_hostname in groups['all']

- debug: msg={{ kill_conmon_podman }}
  when: inventory_hostname in groups['all'] and print_debug == true


- name: Uninstall conmon package
  yum:
    name: "{{ item }}"
    state: absent
  with_items:
    - cephadm
    - ceph-common
    - ceph-base
    - ceph-radosgw
    - conmon
    - podman
  when: inventory_hostname in groups['all']


- name: Reboot required  ( Red Hat ) - Step 1 in RHEL/CentOS/Rocky Linux 8.x and 9.x
  command: /usr/bin/needs-restarting -r
  register: reboot_required
  changed_when: False
  when: ( ansible_distribution == "RedHat" or ansible_distribution == "CentOS" or ansible_distribution == "Rocky" ) and hostvars[inventory_hostname].ansible_distribution_major_version|int >= 8


- name: Reboot required ( Red Hat ) - Step 2
  shell: ( /bin/sleep 5 ; /sbin/shutdown -r now "Ansible updates triggered" ) &
  async: 120
  poll: 0
  notify:
    - Waiting for server to come back after restart
  when: ( ansible_distribution == "RedHat" or ansible_distribution == "CentOS" or ansible_distribution == "Rocky" ) and reboot_required.rc == 1
