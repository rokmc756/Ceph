#
- name: Get fsid for setting fact
  shell: |
    cat /etc/ceph/ceph.conf  | grep fsid | awk '{print $3}'
  register: get_fsid
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ get_fsid.stdout }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Set fact for fsid
  set_fact:
    _fsid: "{{ get_fsid.stdout }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Set fact for cephadm_cmd command
  set_fact:
    cephadm_cmd: "/usr/sbin/cephadm shell --fsid {{ _fsid }} -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Get All OSD IDs
  shell: |
    ceph config dump | grep -E '^osd\.' | awk '{print $1}' | cut -d . -f 2 | sort -nr | tr '\n' ',' | sed 's/,$/\n/'
  register: all_osd_ids
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ all_osd_ids }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- set_fact:
    all_osd: "{{ all_osd_ids.stdout | split(',')}}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ item }}
  with_items: "{{ all_osd }}"
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Unlabel the OSD Nodes with Its Role
  shell: |
    {{ cephadm_cmd }} ceph orch host label rm {{ hostvars[item]['ansible_hostname'] }} {{ hostvars[item]['ansible_hostname'] }}-osd
  register: osd_nodes_unlabled
  ignore_errors: true
  with_items: "{{ groups['osd'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_nodes_unlabled }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # {{ cephadm_cmd }} ceph orch host label rm {{ hostvars[item]['ansible_hostname'] }} {{ hostvars[item]['ansible_hostname'] }}-osd

#
- name: Out OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd out all
  register: osd_out
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_out }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # --yes-i-really-mean-it
  # {{ cephadm_cmd }} ceph osd out {{ item }}

#
- name: Down OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd down all
  register: osd_downed
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_downed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # {{ cephadm_cmd }} ceph osd down {{ item }} --definitely-dead
  # --definitely-dead

#
- name: Remove OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd crush rm osd.{{ item }}
  register: osd_removed
  ignore_errors: true
  with_items: "{{ all_osd }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_removed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Remove Auth of OSDs
  shell: |
    {{ cephadm_cmd }} ceph auth del osd.{{ item }}.
  register: osd_auth_removed
  ignore_errors: true
  with_items: "{{ all_osd }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_auth_removed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name:  Destroy OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd destroy {{ item }} --force
  register: osd_destroyed
  ignore_errors: true
  with_items: "{{ all_osd }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_destroyed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Purge OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd purge {{ item }} --force
  register: osd_purged
  ignore_errors: true
  with_items: "{{ all_osd }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_purged }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # {{ cephadm_cmd }} ceph osd purge {{ item }} --yes-i-really-mean-it


# https://access.redhat.com/documentation/ko-kr/red_hat_ceph_storage/1.2.3/html/red_hat_ceph_administration_guide/removing-osds-manual
- name: Remove the Cluster Hosts and Check if There is Ceph Daemon Running
  shell: |
    {{ cephadm_cmd }} ceph orch host rm {{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch host drain {{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch host rm --force {{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch ps {{ hostvars[item]['ansible_hostname'] }}
  register: cluster_hosts_removed
  ignore_errors: true
  loop: "{{ groups['osd'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ cluster_hosts_removed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # {{ cephadm_cmd }} ceph orch host rm {{ hostvars[item]['ansible_hostname'] }}
  # {{ cephadm_cmd }} ceph orch host drain {{ hostvars[item]['ansible_hostname'] }}
  # {{ cephadm_cmd }} ceph orch ps {{ hostvars[item]['ansible_hostname'] }}
  # {{ cephadm_cmd }} ceph orch host ls
  # {{ cephadm_cmd }} ceph orch ps {{ hostvars[item]['ansible_hostname'] }}

#
- name: Clear Memory Cache
  shell: |
    sync && echo 3 > /proc/sys/vm/drop_caches
  delegate_to: "{{ item }}"
  register: clear_cache_memory
  loop: "{{ groups['osd'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ clear_cache_memory }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Copy Wipe CephFS Script
  template: src=wipe-cephfs.sh.j2 dest=/root/wipe-cephfs.sh owner=root group=root mode=755 force=yes
  register: copy_wipe_cephfs_script
  when: inventory_hostname in groups['osd']
- debug: msg={{ copy_wipe_cephfs_script }}
  when: print_debug == true and inventory_hostname in groups['osd']

#
- name: Run Script to Wipe CephFS
  shell: |
    sh /root/wipe-cephfs.sh
  register: run_wipe_cephfs_script
  when: inventory_hostname in groups['osd']
- debug: msg={{ run_wipe_cephfs_script }}
  when: print_debug == true and inventory_hostname in groups['osd']

#
- name: Reboot Required
  shell: ( /bin/sleep 5 ; /sbin/shutdown -r now "Ansible updates triggered" ) &
  register: reboot_required
  async: 120
  poll: 0
  notify:
    - Waiting for server to come back after restart
  ignore_errors: true
  when: inventory_hostname in groups['osd']

#
- meta: flush_handlers
  when: inventory_hostname in groups['osd']

#
- name: Validate if Devices has still filesytem and is insufficent space
  shell: ceph orch device ls
  register: fs_space_checked
  until: fs_space_checked.stdout.find("Has a FileSystem, Insufficient space") == -1
  retries: 100
  delay: 10
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ fs_space_checked }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # until: fs_space_checked.stderr.find("Has a FileSystem, Insufficient space") == -1

