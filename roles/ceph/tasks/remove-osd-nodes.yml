#
- name: Get FSID for Setting Fact
  shell: |
    cat /etc/ceph/ceph.conf | grep fsid | awk '{print $3}'
  register: get_fsid
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ get_fsid.stdout }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
# ceph osd crush remove <hostname>

#
- name: Set Fact for FSID
  set_fact:
    _fsid: "{{ get_fsid.stdout }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Set fact for cephadm_cmd command
  set_fact:
    cephadm_cmd: "/usr/sbin/cephadm shell --fsid {{ _fsid }} -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Get All OSD IDs
  shell: |
    ceph osd status | sed 1d | awk '{print $1}' | sort -nr | tr '\n' ',' | sed 's/,$/\n/'
  register: all_osd_ids
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ all_osd_ids }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # ceph config dump | grep -E '^osd\.' | grep iops_ssd | awk '{print $1}' | cut -d . -f 2 | sort -nr | tr '\n' ',' | sed 's/,$/\n/'
  #  ceph osd status | grep -E '{{ all_osd_nodes }}' | sed 1d | awk '{print $1}' | sort -nr | tr '\n' ',' | sed 's/,$/\n/'

#
- set_fact:
    all_osd: "{{ all_osd_ids.stdout | split(',')}}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ item }}
  with_items: "{{ all_osd }}"
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Unlabel the OSD Nodes with Its Role
  shell: |
    {{ cephadm_cmd }} ceph orch host label rm {{ hostvars[item]['ansible_hostname'] }} {{ hostvars[item]['ansible_hostname'] }}-osd
  register: osd_nodes_unlabled
  ignore_errors: true
  with_items: "{{ groups['osd'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_nodes_unlabled }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # {{ cephadm_cmd }} ceph orch host label rm {{ hostvars[item]['ansible_hostname'] }} {{ hostvars[item]['ansible_hostname'] }}-osd

#
- name: Disable Scrub for OSDs
  shell: |
    {{ cephadm_cmd }} ceph tell osd.* injectargs --osd-max-backfills 1 --osd-recovery-max-active 1 --osd-recovery-op-priority 1
    {{ cephadm_cmd }} ceph osd set noscrub
    {{ cephadm_cmd }} ceph osd set nodeep-scrub
  register: scrub_disabled
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ scrub_disabled }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
# ceph tell osd.* injectargs --osd-max-backfills 1 --osd-recovery-max-active 1 --osd-recovery-op-priority 1

#
- name: Out OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd out all
  register: osd_out
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_out }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  #  {{ cephadm_cmd }} ceph osd out {{ item }}
  # with_items: "{{ all_osd }}"

#
- name: Down OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd down all
  register: osd_downed
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_downed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # {{ cephadm_cmd }} ceph osd down {{ item }} --definitely-dead
  # with_items: "{{ all_osd }}"

#
- name: Remove Auth of OSDs
  shell: |
    {{ cephadm_cmd }} ceph auth del osd.{{ item }}
  register: osd_auth_removed
  ignore_errors: true
  with_items: "{{ all_osd }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_auth_removed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name:  Destroy OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd destroy {{ item }} --force
  register: osd_destroyed
  ignore_errors: true
  with_items: "{{ all_osd }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_destroyed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Remove OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd rm {{ item }}
  register: osd_removed
  ignore_errors: true
  with_items: "{{ all_osd }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_removed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Remove Crush OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd crush rm osd.{{ item }}
  register: osd_removed
  ignore_errors: true
  with_items: "{{ all_osd }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_removed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Purge OSDs
  shell: |
    {{ cephadm_cmd }} ceph osd purge {{ item }} --force
  register: osd_purged
  ignore_errors: true
  with_items: "{{ all_osd }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_purged }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # {{ cephadm_cmd }} ceph osd purge {{ item }} --yes-i-really-mean-it

#
- name: Remove Host Bucket in Crush Map
  shell: |
    {{ cephadm_cmd }} ceph osd crush rm {{ item }}
  register: bucket_removed
  loop: "{{ groups['osd'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ bucket_removed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Unset Scrub for OSDs
  shell: |
    {{ cephadm_cmd }} ceph tell osd.* injectargs --osd-max-backfills 1 --osd-recovery-max-active 3 --osd-recovery-op-priority 3
    {{ cephadm_cmd }} ceph osd unset noscrub
    {{ cephadm_cmd }} ceph osd unset nodeep-scrub
    {{ cephadm_cmd }} ceph osd unset noup
  register: scrub_unset
  ignore_errors: true
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ scrub_unset }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

# https://access.redhat.com/documentation/ko-kr/red_hat_ceph_storage/1.2.3/html/red_hat_ceph_administration_guide/removing-osds-manual
- name: Delete OSD Users
  shell: |
    {{ cephadm_cmd }} ceph auth del client.ceph-exporter.{{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph auth del client.crash.{{ hostvars[item]['ansible_hostname'] }}
  register: osd_users_deleted
  ignore_errors: true
  loop: "{{ groups['osd'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ osd_users_deleted }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

# ceph orch start node-exporter.rk9-node04
# ceph orch redeploy node-exporter.rk9-node04
# ceph orch redeploy node-exporter
# ceph orch daemon restart node-exporter.rk9-node06
# ceph orch daemon restart ceph-exporter.rk9-node05
# ceph orch daemon restart crash.rk9-node06

# https://access.redhat.com/documentation/ko-kr/red_hat_ceph_storage/1.2.3/html/red_hat_ceph_administration_guide/removing-osds-manual
- name: Remove the Cluster Hosts and Check if There is Ceph Daemon Running
  shell: |
    {{ cephadm_cmd }} ceph orch host rm {{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch host drain {{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch host rm --force {{ hostvars[item]['ansible_hostname'] }}
    {{ cephadm_cmd }} ceph orch ps {{ hostvars[item]['ansible_hostname'] }}
  register: cluster_hosts_removed
  ignore_errors: true
  loop: "{{ groups['osd'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ cluster_hosts_removed }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
  # {{ cephadm_cmd }} ceph orch host ls

#
#- name: Stop and Remove Ceph and Node Exporters's Containers
#  shell: |
#    for i in `systemctl list-units '*ceph*' | grep running | awk '{print $1}'`; do systemctl stop $i && systemctl disable $i ; done
#  ignore_errors: true
#  register: exporter_containers_stopped
#  when: inventory_hostname in groups['osd']
#- debug: msg={{ exporter_containers_stopped }}
#  when: print_debug == true and inventory_hostname in groups['osd']
# https://access.redhat.com/documentation/ko-kr/red_hat_ceph_storage/4/html/operations_guide/removing-a-ceph-osd-node_ops
# https://www.ibm.com/docs/en/storage-ceph/5?topic=failure-removing-ceph-osd-node
# podman images | awk '{print $3}'
# podman rmi -f 1c40e0e88d74


# https://access.redhat.com/documentation/ko-kr/red_hat_ceph_storage/1.2.3/html/red_hat_ceph_administration_guide/removing-osds-manual
#- name: Delete OSD Users
#  shell: |
#    {{ cephadm_cmd }} ceph auth del client.ceph-exporter.{{ hostvars[item]['ansible_hostname'] }}
#    {{ cephadm_cmd }} ceph auth del client.crash.{{ hostvars[item]['ansible_hostname'] }}
#  register: osd_users_deleted
#  ignore_errors: true
#  loop: "{{ groups['osd'] }}"
#  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
#- debug: msg={{ osd_users_deleted }}
#  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
#- name: Remove FSID Directory
#  shell: |
#    rm -rf "/var/lib/ceph/{{ _fsid }}"
#  ignore_errors: true
#  register: fsid_dir_removed
#  when: inventory_hostname in groups['osd']
#- debug: msg={{ fsid_dir_removed }}
#  when: print_debug == true and inventory_hostname in groups['osd']
# https://access.redhat.com/documentation/ko-kr/red_hat_ceph_storage/4/html/operations_guide/removing-a-ceph-osd-node_ops
# https://www.ibm.com/docs/en/storage-ceph/5?topic=failure-removing-ceph-osd-node

#
- name: Clear Memory Cache
  shell: |
    sync && echo 3 > /proc/sys/vm/drop_caches
  delegate_to: "{{ item }}"
  register: clear_cache_memory
  loop: "{{ groups['osd'] }}"
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ clear_cache_memory }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

#
- name: Copy Wipe CephFS Script
  template: src=wipe-cephfs.sh.j2 dest=/root/wipe-cephfs.sh owner=root group=root mode=755 force=yes
  ignore_errors: true
  register: copy_wipe_cephfs_script
  when: inventory_hostname in groups['osd']
- debug: msg={{ copy_wipe_cephfs_script }}
  when: print_debug == true and inventory_hostname in groups['osd']

#
- name: Run Script to Wipe CephFS
  shell: |
    sh /root/wipe-cephfs.sh
  ignore_errors: true
  register: run_wipe_cephfs_script
  when: inventory_hostname in groups['osd']
- debug: msg={{ run_wipe_cephfs_script }}
  when: print_debug == true and inventory_hostname in groups['osd']

#
- name: Reboot Required
  shell: ( /bin/sleep 5; /sbin/shutdown -r now "Ansible updates triggered" ) &
  ignore_errors: true
  register: reboot_required
  async: 120
  poll: 0
  notify:
    - Waiting for server to come back after restart
  when: inventory_hostname in groups['all']

#
- meta: flush_handlers
  when: inventory_hostname in groups['all']

# ceph orch device ls | grep -E '{{ all_osd_nodes }}'
- name: Validate if Devices has still filesytem and is insufficent space
  shell: |
    ceph orch device ls
  register: fs_space_checked
  ignore_errors: true
  until: fs_space_checked.stdout.find("Has a FileSystem, Insufficient space") == -1
  retries: 100
  delay: 10
  when: inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']
- debug: msg={{ fs_space_checked }}
  when: print_debug == true and inventory_hostname in hostvars[groups['mon'][0]]['ansible_hostname']

