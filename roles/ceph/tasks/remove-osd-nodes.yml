# https://access.redhat.com/documentation/ko-kr/red_hat_ceph_storage/4/html/operations_guide/removing-a-ceph-osd-node_ops
# https://www.ibm.com/docs/en/storage-ceph/5?topic=failure-removing-ceph-osd-node


- name: Clear Memory Cache
  shell: |
    sync && echo 3 > /proc/sys/vm/drop_caches
  register: clear_cache_memory
- debug: msg={{ clear_cache_memory }}
  when: print_debug == true


#- name: Copy Wipe CephFS Script
#  template: src=wipe-cephfs.sh.j2 dest=/root/wipe-cephfs.sh owner=root group=root mode=755 force=yes
#  ignore_errors: true
#  register: copy_wipe_cephfs_script
#- debug: msg={{ copy_wipe_cephfs_script }}
#  when: print_debug == true


#- name: Run Script to Wipe CephFS
#  shell: |
#    sh /root/wipe-cephfs.sh
#  ignore_errors: true
#  register: run_wipe_cephfs_script
#- debug: msg={{ run_wipe_cephfs_script }}
#  when: print_debug == true


- name: Remove LVMs
  shell: |
    for vg in $(`vgs | grep ceph | awk '{print $1}'`); do vgremove $vg -f ;done
    for dm in $(fdisk -l | grep mapper | grep ceph | awk '{print $2}' | cut -d : -f 1); do dmsetup remove $dm ;done
  register: remove_lvms
  ignore_errors: true
- debug: msg={{ remove_lvms }}
  when: print_debug == true


- name: Cleanup the Ceph Block Devices
  shell: |
    pvremove -y -ff {{ item }}
    dd if=/dev/zero of={{ item }} bs=4096 count=1 conv=notrunc
    nvme format {{ item }}
    wipefs -a {{ item }}
    sgdisk --zap-all --delete {{ item }}
    blkdiscard {{ item }}
  register: cleanup_ceph_block_devices
  ignore_errors: true
  with_items: "{{ nvme_device_array }}"
- debug: msg={{ cleanup_ceph_block_devices }}
  when: print_debug == true


- name: Reboot Required
  shell: ( /bin/sleep 5; /sbin/shutdown -r now "Ansible Updates Triggered" ) &
  ignore_errors: true
  register: reboot_required
  async: 120
  poll: 0
  notify:
    - Waiting for Server to Come Back after Restart


- meta: flush_handlers

