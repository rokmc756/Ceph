---
- name: Create Ceph Filesystem
  shell: |
    ceph fs new {{ item.name }}-fs01 {{ item.name }}_metadata {{ item.name }}_data
  register: cephfs_created
  with_items: "{{ _cephfs.pool }}"
- debug: msg={{ cephfs_created }}
  when: print_debug == true


#- name: Create Ceph Filesystem Sub Volumes
#  shell: |
#    ceph fs subvolume create {{ item.name }} {{ item.name }}-subvol01
#  register: cephfs_subvol_created
#  with_items: "{{ cephfs.fs }}"
#- debug: msg={{ cephfs_subvol_created }}
#  when: print_debug == true


# Note that Ceph only increases the actual number of ranks in the Ceph File Systems if a spare MDS daemon is available to take the new rank.
- name: Set the max_mds Parameter to the Desired Number of Active MDS Daemons
  shell: |
    ceph fs set {{ item.name }} max_mds 3
  register: active_mds_daemon_set
  with_items: "{{ _cephfs.fs }}"
- debug: msg={{ active_mds_daemon_set }}
  when: print_debug == true
# ceph mds deactivate cephfs:1


- name: Verify the Number of Active MDS Daemons
  shell: |
    ceph fs status {{ item.name }}
  register: active_mds_daemons_verified
  with_items: "{{ _cephfs.fs }}"
- debug: msg={{ active_mds_daemons_verified }}
  when: print_debug == true


- name: Authorize CephFS Client to Access New Ceph Filesystem
  shell: |
    ceph fs authorize {{ item.name }} client.{{ item.name }} / rw root_squash / rw
  register: cephfs_client_authorized
  with_items: "{{ _cephfs.fs }}"
- debug: msg={{ cephfs_client_authorized }}
  when: print_debug == true
# ceph fs authorize {{ item.name }} client.{{ item.name }} /temp rw
# ceph fs authorize {{ item.name }} client.{{ item.name }} / r /temp rw


- name: Print Authentication Key for Ceph Users
  shell: |
    ceph auth get client.{{ item.name }}
  register: ceph_user_key_printed
  with_items: "{{ _cephfs.fs }}"
- debug: msg={{ ceph_user_key_printed }}
  when: print_debug == true


- name: Copy Authentication Key into Ceph Client Nodes for Ceph Users
  shell: |
    ceph auth get client.{{ item.name }} > /etc/ceph/ceph.client.{{ item.name }}.keyring
  register: ceph_user_auth_key_copied
  with_items: "{{ _cephfs.fs }}"
- debug: msg={{ ceph_user_auth_key_copied }}
  when: print_debug == true


- name: Check Active Stat
  shell: |
    ceph mds stat
  register: active_stats_checked
- debug: msg={{ active_stats_checked }}
  when: print_debug == true


#- name: Erasure Coded pools as CephFS data pools as long as they have overwrites enabled
#  shell: |
#    ceph osd pool set {{ item.name }}_data allow_ec_overwrites true
#    ceph osd pool set {{ item.name }}_metadata allow_ec_overwrites true
#  register: ec_overwrites_allowed
#  with_items: "{{ cephfs.pool }}"
#- debug: msg={{ ec_overwries_allowed }}
#  when: print_debug == true
#  "start": "2024-05-01 07:43:42.711158", "stderr": "Error EINVAL: ec overwrites can only
#  be enabled for an erasure coded pool\nError EINVAL: ec overwrites can only be enabled
#  for an erasure coded pool", "stderr_lines": ["Error EINVAL: ec overwrites can only be
#  enabled for an erasure coded pool", "Error EINVAL: ec overwrites can only be enabled
#  for an erasure coded pool"], "stdout": "", "stdout_lines": []}


# https://docs.ceph.com/en/quincy/cephfs/fs-volumes/
# https://www.ibm.com/docs/en/storage-ceph/6?topic=subvolumes-creating-file-system-subvolume
# https://access.redhat.com/documentation/th-th/red_hat_ceph_storage/4/html/file_system_guide/management-of-ceph-file-system-volumes-subvolumes-and-subvolume-groups
# https://www.ibm.com/docs/en/storage-ceph/6?topic=subvolumes-creating-snapshot-file-system-subvolume
# https://www.ibm.com/docs/en/storage-ceph/6?topic=system-creating-ceph-file-systems
# https://access.redhat.com/documentation/ko-kr/red_hat_ceph_storage/5/html/file_system_guide/creating-ceph-file-systems_fs

