#
- name: Get FSID
  shell: |
    cat /etc/ceph/ceph.conf | grep fsid | awk '{print $3}'
  register: get_fsid
- debug: msg={{ get_fsid.stdout }}
  when: print_debug == true

#
- name: Set Fact for FSID
  set_fact:
    _fsid: "{{ get_fsid.stdout }}"

#
- name: Set Fact for cephadm_cmd command
  set_fact:
    cephadm_cmd: "/usr/sbin/cephadm shell --fsid {{ _fsid }} -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --"

#
- name: Create a NFS filesystem Volume
  shell: |
    ceph fs volume create {{ item.fs }}
  register: create_nfs_vol
  with_items: "{{ nfs.ganesha }}"
- debug: msg={{ create_nfs_vol }}
  when: print_debug == true

#
- name: Enable the Ceph Manager NFS module
  shell: |
    {{ cephadm_cmd }} ceph mgr module enable nfs
  register: nfs_module_enabled
- debug: msg={{ nfs_module_enabled }}
  when: print_debug == true

#
- name: Create RADOS Pools
  shell: |
    {{ cephadm_cmd }} ceph osd pool create {{ item.pool }}
  register: rados_pools_created
  with_items: "{{ nfs.ganesha }}"
- debug: msg={{ rados_pools_created }}
  when: print_debug == true

#
- name: Enable Application for NFS
  shell: |
    {{ cephadm_cmd }} ceph osd pool application enable {{ item.pool }} nfs
  register: nfs_app_enabled
  with_items: "{{ nfs.ganesha }}"
- debug: msg={{ nfs_app_enabled }}
  when: print_debug == true

#
- name: Initialize RBD Pool
  shell: |
    {{ cephadm_cmd }} rbd pool init -p {{ item.pool }}
  register: rbd_pool_init
  with_items: "{{ nfs.ganesha }}"
- debug: msg={{ rbd_pool_init }}
  when: print_debug == true

#
#- name: Deploy NFS-Ganesha Gateway Using Placement Specification
#  shell: |
#    {{ cephadm_cmd }} ceph orch apply nfs {{ item.name }} --placement="{{ ceph.mon_host_num }} {{ all_mon_hostnames }} "
#  register: nfs_ganesha_gw_deployed
#  with_items: "{{ nfs.ganesha }}"
#- debug: msg={{ nfs_ganesha_gw_deployed }}
#  when: print_debug == true

#
- name: Deploy NFS-Ganesha Cluster Using Command Line
  shell: |
    {{ cephadm_cmd }} ceph nfs cluster create {{ item.name }} "{{ ceph.mon_host_num }} {{ all_mon_hostnames }}" --ingress --virtual-ip {{ item.virtual_ip }}
  register: nfs_ganesha_gw_deployed
  with_items: "{{ nfs.ganesha }}"
- debug: msg={{ nfs_ganesha_gw_deployed }}
  when: print_debug == true

#
- name: Check if NFS Ganesha Serivce is still Starting
  shell: |
    ceph orch ps --daemon_type=nfs
  register: nfs_ganesha_status
  until: nfs_ganesha_status.stdout.find("starting") == -1
  retries: 20
  delay: 15
- debug: msg={{ nfs_ganesha_status }}
  when: print_debug == true

#
- name: Check if NFS Ganesha Serivce is still Starting
  shell: |
    ceph orch ps --daemon_type=nfs
  register: nfs_ganesha_status
  until: nfs_ganesha_status.stdout.find("unknown") == -1
  retries: 20
  delay: 15
- debug: msg={{ nfs_ganesha_status }}
  when: print_debug == true

#
- name: List Services
  shell: |
    {{ cephadm_cmd }} ceph orch ls
    {{ cephadm_cmd }} ceph orch ps --daemon_type=nfs
  register: services_listed
- debug: msg={{ services_listed }}
  when: print_debug == true

#
- name: Check the Cluster Info Status when Creating NFS
  shell: |
    ceph nfs cluster info {{ item.name }}
  register: check_nfs_clu_info
  with_items: "{{ nfs.ganesha }}"
- debug: msg={{ check_nfs_clu_info }}
  when: print_debug == true

#
- name: Check the ls status when Creating NFS Ganesha Cluster
  shell: |
    ceph orch ls --service_name=nfs.{{ item.name }}
    ceph orch ps --service_name=nfs.{{ item.name }}
  register: check_nfs_ls
  with_items: "{{ nfs.ganesha }}"
- debug: msg={{ check_nfs_ls }}
  when: print_debug == true

- name: Copy Ganesha Configuration File
  template: src=ceph-nfs.conf.j2 dest={{ ceph.base_path }}/ceph-nfs.conf owner=root group=root mode=644 force=yes
  register: ganesha_config_copied
- debug: msg={{ ganesha_config_copied }}
  when: print_debug == true

#
- name: Create a Configuration File and Apply it to the NFS Cluster
  shell: |
    ceph nfs cluster config set {{ item.name }} -i /root/ceph-nfs.conf
  ignore_errors: true
  register: nfs_config_created
  with_items: "{{ nfs.ganesha }}"
- debug: msg={{ nfs_config_created }}
  when: print_debug == true

#
- name: View customized NFS Ganesha Configuration
  shell: |
    ceph nfs cluster config get {{ item.cluster }}
  register: customized_nfs_ganesha_config_viewed
  with_items: "{{ nfs.export }}"
- debug: msg={{ customized_nfs_ganesha_config_viewed }}
  when: print_debug == true

#
- name: Create a User, nfsclient to Expose CephFS though the NFS server
  shell: |
    ceph auth {{ item.action }} client.{{ item.user }} mon '{{ item.mon }}' \
    osd '{{ item.osd }} pool={{ item.pool }} namespace={{ item.namespace }}, {{ item.cephfs }} data={{ item.data }}' \
    mds '{{ item.mds }} path={{ item.path }}' > {{ item.key }}
  ignore_errors: true
  register: create_nfs_auth
  with_items: "{{ nfs.auth }}"
- debug: msg={{ create_nfs_auth }}
  when: print_debug == true
#  https://docs.ceph.com/en/quincy/mgr/nfs/


# Export RGW Bucket
# ceph nfs export create rgw --cluster-id <cluster_id> --pseudo-path <pseudo_path> --bucket <bucket_name>
# [--user-id <user-id>] [--readonly] [--client_addr <value>...] [--squash <value>] [--sectype <value>...]
# ceph nfs export create rgw --cluster-id mynfs --pseudo-path /bucketdata --bucket mybucket --client_addr 192.168.10.0/24
#
# RGW USER EXPORTÔÉÅ
# ceph nfs export create rgw --cluster-id mynfs --pseudo-path /bucketdata --user-id myuser --client_addr 192.168.10.0/24
#
# Delete EXPORT
# ceph nfs export rm <cluster_id> <pseudo_path>


#
- name: Create a CephFS Export
  shell: |
    ceph nfs export create cephfs {{ item.cluster }} {{ item.src }} {{ item.fs }} --path={{ item.dest }}
  register: create_cephfs_export
  with_items: "{{ nfs.export }}"
- debug: msg={{ create_cephfs_export }}
  when: print_debug == true
  # ceph nfs export create {{ item.name }} {{ item.cluster }} {{ item.src }} {{ item.fs }} --path={{ item.dest }}
#  # ceph fs subvolume getpath cephfs sub0

#
# https://github.com/nfs-ganesha/nfs-ganesha/blob/next/src/config_samples/export.txt
# ceph nfs export create cephfs {{ item.cluster }} {{ item.src }} {{ item.fs }} --path={{ item.dest }}
#- name: Create a CephFS Export
#  shell: |
#    ceph nfs export create cephfs --cluster-id {{ item.cluster }} --pseudo-path {{ item.src }} --fsname {{ item.fs }} \
#    --path={{ item.dest }} --client_addr 192.168.1.00/24 --squash no_root_squash
#  register: create_cephfs_export
#  with_items: "{{ nfs.export }}"
#- debug: msg={{ create_cephfs_export }}
#  when: print_debug == true
# ceph fs subvolume getpath <vol_name> <subvol_name> [--group_name <subvol_group_name>]

# ceph nfs export create rgw --cluster-id mynfs --pseudo-path /bucketdata --bucket mybucket --client_addr 192.168.10.0/24
# ceph nfs export create rgw --cluster-id mynfs --pseudo-path /bucketdata --user-id myuser --client_addr 192.168.10.0/24

#
- name: List the NFS exports
  shell: |
    ceph nfs export ls {{ item.cluster }}
  register: list_nfs_export
  with_items: "{{ nfs.export }}"
- debug: msg={{ list_nfs_export }}
  when: print_debug == true

#
- name: Get the Information of the NFS Export
  shell: |
    ceph nfs cluster info {{ item.cluster }}
  register: get_nfs_export
  with_items: "{{ nfs.export }}"
- debug: msg={{ get_nfs_export }}
  when: print_debug == true

#
- name: Pause until Pools are started to Scrub
  pause:
    seconds: 40

#
- name: Check if Pools are still Unknown
  shell: |
    ceph pg stat
  register: unknown_pool_checked
  until: unknown_pool_checked.stdout.find("unknown") == -1
  retries: 30
  delay: 10
- debug: msg={{ unknown_pool_checked }}
  when: print_debug == true

#
- name: Check if Pools are Active+Clean
  shell: |
    ceph pg stat
  register: scrub_pool_checked
  until: scrub_pool_checked.stdout.find("scrubbing") == -1
  retries: 20
  delay: 15
- debug: msg={{ scrub_pool_checked }}
  when: print_debug == true

# ceph osd map cephfs.jtest-fs01.data jtest-ns01
# ceph health detail
# HEALTH_WARN 1 failed cephadm daemon(s)


# ceph nfs cluster info
#{
#  "jtest-clu01": {
#    "backend": [
#      {
#        "hostname": "rk9-node01",
#        "ip": "192.168.1.71",
#        "port": 2049
#      },
#      {
#        "hostname": "rk9-node02",
#        "ip": "192.168.0.72",
#        "port": 2049
#      },
#      {
#        "hostname": "rk9-node03",
#        "ip": "192.168.0.73",
#        "port": 2049
#      }
#    ],
#    "virtual_ip": null
#  }
#}

# ceph orch ps --daemon_type=nfs
#NAME                                   HOST        PORTS   STATUS  REFRESHED  AGE  MEM USE  MEM LIM  VERSION    IMAGE ID
#nfs.jtest-clu01.0.0.rk9-node03.vdmxmn  rk9-node03  *:2049  error      4m ago  35m        -        -  <unknown>  <unknown>
#nfs.jtest-clu01.1.0.rk9-node02.rwqmyc  rk9-node02  *:2049  error      4m ago  35m        -        -  <unknown>  <unknown>
#nfs.jtest-clu01.2.0.rk9-node01.fohytq  rk9-node01  *:2049  error      4m ago  35m        -        -  <unknown>  <unknown>

