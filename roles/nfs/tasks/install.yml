#
- name: Get FSID
  shell: |
    cat /etc/ceph/ceph.conf | grep fsid | awk '{print $3}'
  register: get_fsid
- debug: msg={{ get_fsid.stdout }}
  when: print_debug == true

#
- name: Set Fact for FSID
  set_fact:
    _fsid: "{{ get_fsid.stdout }}"

#
- name: Set Fact for cephadm_cmd command
  set_fact:
    cephadm_cmd: "/usr/sbin/cephadm shell --fsid {{ _fsid }} -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --"

#
- name: Create a NFS filesystem Volume
  shell: |
    ceph fs volume create {{ item.fs }}
  register: create_nfs_vol
  with_items: "{{ nfs.cluster }}"
- debug: msg={{ create_nfs_vol }}
  when: print_debug == true

#
- name: Enable the Ceph Manager NFS module
  shell: |
    ceph mgr module enable nfs
  register: nfs_module_enabled
- debug: msg={{ nfs_module_enabled }}
  when: print_debug == true

#
- name: Create NFS Ganesha Cluster
  shell: |
    ceph nfs cluster create {{ item.name }} "{{ all_mon_hostnames }}"
  register: create_nfs
  with_items: "{{ nfs.cluster }}"
- debug: msg={{ create_nfs }}
  when: print_debug == true

#
- name: Check the Cluster Info Status when Creating NFS
  shell: |
    ceph nfs cluster info {{ item.name }}
  register: check_nfs_clu_info
  with_items: "{{ nfs.cluster }}"
- debug: msg={{ check_nfs_clu_info }}
  when: print_debug == true

#
- name: Check the ls status when Creating NFS Ganesha Cluster
  shell: |
    ceph orch ls --service_name=nfs.{{ item.name }}
  register: check_nfs_ls
  with_items: "{{ nfs.cluster }}"
- debug: msg={{ check_nfs_ls }}
  when: print_debug == true

#
- name: Check the ps status when Creating NFS Ganesha Cluster
  shell: |
    ceph orch ps --service_name=nfs.{{ item.name }}
  register: check_nfs_ps
  with_items: "{{ nfs.cluster }}"
- debug: msg={{ check_nfs_ps }}
  when: print_debug == true

#
- name: Create a User, nfsclient to Expose CephFS though the NFS server
  shell: |
    ceph auth {{ item.action }} client.{{ item.user }} mon '{{ item.mon }}' \
    osd '{{ item.osd }} pool={{ item.pool }} namespace={{ item.namespace }}, {{ item.cephfs }} data={{ item.data }}' \
    mds '{{ item.mds }} path={{ item.path }}' > {{ item.key }}
  ignore_errors: true
  register: create_nfs_auth
  with_items: "{{ nfs.auth }}"
- debug: msg={{ create_nfs_auth }}
  when: print_debug == true

#
- name: Create a Configuration File and Apply it to the NFS Cluster
  shell: |
    ceph nfs cluster config set {{ item.name }} -i /root/nfs.conf
  ignore_errors: true
  register: create_nfs_config
  with_items: "{{ nfs.cluster }}"
- debug: msg={{ create_nfs_config }}
  when: print_debug == true

#
- name: View the Export Block Based on the Pseudo Root Name
  shell: |
    ceph nfs export get {{ item.cluster }} {{ item.src }}
  register: view_nfs_export
  with_items: "{{ nfs.export }}"
- debug: msg={{ view_nfs_export }}
  when: print_debug == true

#
- name: Create a CephFS Export
  shell: |
    ceph nfs export create cephfs {{ item.cluster }} {{ item.src }} {{ item.fs }} --path={{ item.dest }}
  register: create_cephfs_export
  with_items: "{{ nfs.export }}"
- debug: msg={{ create_cephfs_export }}
  when: print_debug == true
  # ceph nfs export create {{ item.name }} {{ item.cluster }} {{ item.src }} {{ item.fs }} --path={{ item.dest }}
  # ceph fs subvolume getpath cephfs sub0

#
- name: List the NFS exports
  shell: |
    ceph nfs export ls {{ item.cluster }}
  register: list_nfs_export
  with_items: "{{ nfs.export }}"
- debug: msg={{ list_nfs_export }}
  when: print_debug == true

#
- name: Get the Information of the NFS Export
  shell: |
    ceph nfs cluster info {{ item.cluster }}
  register: get_nfs_export
  with_items: "{{ nfs.export }}"
- debug: msg={{ get_nfs_export }}
  when: print_debug == true

#
- name: Pause until Pools are started to Scrub
  pause:
    seconds: 40

#
- name: Check if Pools are still Unknown
  shell: |
    ceph pg stat
  register: unknown_pool_checked
  until: unknown_pool_checked.stdout.find("unknown") == -1
  retries: 30
  delay: 10
- debug: msg={{ unknown_pool_checked }}
  when: print_debug == true

#
- name: Check if Pools are Active+Clean
  shell: |
    ceph pg stat
  register: scrub_pool_checked
  until: scrub_pool_checked.stdout.find("scrubbing") == -1
  retries: 20
  delay: 15
- debug: msg={{ scrub_pool_checked }}
  when: print_debug == true

# ceph osd map cephfs.jtest-fs01.data jtest-ns01
# ceph health detail
# HEALTH_WARN 1 failed cephadm daemon(s)


# ceph nfs cluster info
#{
#  "jtest-clu01": {
#    "backend": [
#      {
#        "hostname": "rk9-node01",
#        "ip": "192.168.1.71",
#        "port": 2049
#      },
#      {
#        "hostname": "rk9-node02",
#        "ip": "192.168.0.72",
#        "port": 2049
#      },
#      {
#        "hostname": "rk9-node03",
#        "ip": "192.168.0.73",
#        "port": 2049
#      }
#    ],
#    "virtual_ip": null
#  }
#}

# ceph orch ps --daemon_type=nfs
#NAME                                   HOST        PORTS   STATUS  REFRESHED  AGE  MEM USE  MEM LIM  VERSION    IMAGE ID
#nfs.jtest-clu01.0.0.rk9-node03.vdmxmn  rk9-node03  *:2049  error      4m ago  35m        -        -  <unknown>  <unknown>
#nfs.jtest-clu01.1.0.rk9-node02.rwqmyc  rk9-node02  *:2049  error      4m ago  35m        -        -  <unknown>  <unknown>
#nfs.jtest-clu01.2.0.rk9-node01.fohytq  rk9-node01  *:2049  error      4m ago  35m        -        -  <unknown>  <unknown>

