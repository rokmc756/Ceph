# https://www.ibm.com/docs/en/storage-ceph/5?topic=gateway-deploying-multi-site-ceph-object
#
# Deploying a multi-site Ceph Object Gateway
#
# Ceph Orchestrator supports multi-site configuration options for the Ceph Object Gateway.
# 3You can configure each object gateway to work in an active-active zone configuration allowing writes to a non-primary zone.
# The multi-site configuration is stored within a container called a realm.
# The realm stores zone groups, zones, and a time period. The rgw daemons handle the synchronization eliminating the need for a separate synchronization agent,
# thereby operating with an active-active configuration.
# You can also deploy multi-site zones using the command line interface (CLI).
# NOTE: The following configuration assumes at least two IBM Storage Ceph clusters are in geographically separate locations.
# However, the configuration also works on the same site.

# Parent topic:
# Management of Ceph object gateway
# Prerequisites
# At least two running IBM Storage Ceph clusters.

# At least two Ceph Object Gateway instances, one for each IBM Storage Ceph cluster.
# Root-level access to all the nodes.
# Nodes or containers are added to the storage cluster.
# All Ceph Manager, Monitor and OSD daemons are deployed.

# Procedure
# In the cephadm shell, configure the primary zone:

# [ Create a realm ]
# radosgw-admin realm create --rgw-realm=test_realm --default

# If the storage cluster has a single realm, then specify the --default flag.
# [ Create a primary zone group ]
# radosgw-admin zonegroup create --rgw-zonegroup=us --endpoints=http://rgw1:80 --master --default

# [ Create a primary zone ]
# radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 --endpoints=http://rgw1:80 \
# --access-key=changeme --secret-key=changeme
# Optional: Delete the default zone, zone group, and the associated pools.
# IMPORTANT: Do not delete the default zone and its pools if you are using the default zone and zone group to store data. Also, removing the default zone group deletes the system user.

# To access old data in the default zone and zonegroup, use --rgw-zone default and --rgw-zonegroup default in radosgw-admin commands.
# radosgw-admin zonegroup delete --rgw-zonegroup=default
# ceph osd pool rm default.rgw.log default.rgw.log --yes-i-really-really-mean-it
# ceph osd pool rm default.rgw.meta default.rgw.meta --yes-i-really-really-mean-it
# ceph osd pool rm default.rgw.control default.rgw.control --yes-i-really-really-mean-it
# ceph osd pool rm default.rgw.data.root default.rgw.data.root --yes-i-really-really-mean-it
# ceph osd pool rm default.rgw.gc default.rgw.gc --yes-i-really-really-mean-it

# [ Create a system user ]
# radosgw-admin user create --uid=zone.user --display-name="Zone user" --system
#
# Make a note of the access_key and secret_key.
# Add the access key and system key to the primary zone:
# radosgw-admin zone modify --rgw-zone=us-east-1 --access-key=changeme --secret=changeme

# radosgw-admin period update --commit

# Outside the cephadm shell, fetch the FSID of the storage cluster and the processes:

# [ Start the Ceph Object Gateway daemon ]
# systemctl start ceph-62a081a6-88aa-11eb-a367-001a4a000672@rgw.test_realm.us-east-1.host01.ahdtsw.service
# systemctl enable ceph-62a081a6-88aa-11eb-a367-001a4a000672@rgw.test_realm.us-east-1.host01.ahdtsw.service

# In the Cephadm shell, configure the secondary zone.
# [ Pull the primary realm configuration from the host ]
# radosgw-admin realm pull --url=http://10.74.249.26:80 --access-key=changeme --secret-key=changeme

# [ Pull the primary period configuration from the host ]
# radosgw-admin period pull --url=http://10.74.249.26:80 --access-key=changeme --secret-key=changeme

# [ Configure a secondary zone ]
# radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-2 --endpoints=http://rgw2:80 --access-key=LIPEYZJLTWXRKXS9LPJC \
# --secret-key=IsAje0AVDNXNw48LjMAimpCpI7VaxJYSnfD0FFKQ --endpoints=http://rgw.example.com:80

# Optional: Delete the default zone.
# IMPORTANT: Do not delete the default zone and its pools if you are using the default zone and zone group to store data.
# To access old data in the default zone and zonegroup, use --rgw-zone default and --rgw-zonegroup default in radosgw-admin commands.
# radosgw-admin zone rm --rgw-zone=default
# ceph osd pool rm default.rgw.log default.rgw.log --yes-i-really-really-mean-it
# ceph osd pool rm default.rgw.meta default.rgw.meta --yes-i-really-really-mean-it
# ceph osd pool rm default.rgw.control default.rgw.control --yes-i-really-really-mean-it
# ceph osd pool rm default.rgw.data.root default.rgw.data.root --yes-i-really-really-mean-it
# ceph osd pool rm default.rgw.gc default.rgw.gc --yes-i-really-really-mean-it

# [ Update the Ceph configuration database ]
# ceph config set rgw rgw_zone us-east-2

# [ Commit the changes ]
# radosgw-admin period update --commit

# Outside the Cephadm shell, fetch the FSID of the storage cluster and the processes:
#  systemctl list-units | grep ceph

# [ Start the Ceph Object Gateway daemon ]
# systemctl start ceph-62a081a6-88aa-11eb-a367-001a4a000672@rgw.test_realm.us-east-2.host04.ahdtsw.service
# systemctl enable ceph-62a081a6-88aa-11eb-a367-001a4a000672@rgw.test_realm.us-east-2.host04.ahdtsw.service

# Optional: Deploy multi-site Ceph Object Gateways using the placement specification:
# ceph orch apply rgw east --realm=test_realm --zone=us-east-1 --placement="2 host01 host02"

# Verification
# [ Check the synchronization status to verify the deployment ]
# radosgw-admin sync status

